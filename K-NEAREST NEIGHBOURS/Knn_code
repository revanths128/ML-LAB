import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from scipy.stats import mode
import matplotlib.pyplot as mtp


# K Nearest Neighbors Classification


class K_Nearest_Neighbors_Classifier():

    def __init__(self, K):
        self.K = K

    # Function to store training set

    def fit(self, X_train, Y_train):
        self.X_train = X_train
        self.Y_train = Y_train
        # no_of_training_examples, no_of_features
        self.m, self.n = X_train.shape

    # Function for prediction

    def predict(self, X_test):
        self.X_test = X_test
        # no_of_test_examples, no_of_features
        self.m_test, self.n = X_test.shape
        # initialize Y_predict
        Y_predict = np.zeros(self.m_test)

        for i in range(self.m_test):
            x = self.X_test[i]
            # find the K nearest neighbors from current test example
            neighbors = np.zeros(self.K)
            neighbors = self.find_neighbors(x)
            # most frequent class in K neighbors
            Y_predict[i] = mode(neighbors)[0][0]
        return Y_predict

    # Function to find the K nearest neighbors to current test example

    def find_neighbors(self, x):
        # calculate all the euclidean distances between current
        # test example x and training set X_train
        euclidean_distances = np.zeros(self.m)
        for i in range(self.m):
            d = self.euclidean(x, self.X_train[i])
            euclidean_distances[i] = d
        # sort Y_train according to euclidean_distance_array and
        # store into Y_train_sorted
        inds = euclidean_distances.argsort()
        Y_train_sorted = self.Y_train[inds]
        return Y_train_sorted[:self.K]

    # Function to calculate euclidean distance

    def euclidean(self, x, x_train):
        return np.sqrt(np.sum(np.square(x - x_train)))


# Importing dataset
df = pd.read_csv("diabetes.csv")
X = df[['Age', 'Glucose']].to_numpy()
Y = df['Outcome'].to_numpy()
# Splitting dataset into train and test set
X_train, X_test, Y_train, Y_test = train_test_split(
    X, Y, test_size=0.2, random_state=0)
# Model training
model = K_Nearest_Neighbors_Classifier(K=3)
model.fit(X_train, Y_train)
# Prediction on test set
Y_pred = model.predict(X_test)
# measure performance
correctly_classified = 0
# counter
count = 0

for count in range(np.size(Y_pred)):
    if Y_test[count] == Y_pred[count]:
        correctly_classified = correctly_classified + 1
    count = count + 1
print("Accuracy on test set by our model       :  ", (
        correctly_classified / count) * 100)

from sklearn.metrics import confusion_matrix

cm = confusion_matrix(Y_test, Y_pred)
cm

# Visulaizing the trianing set result
from matplotlib.colors import ListedColormap

x_set, y_set = X_train, Y_train
x1, x2 = np.meshgrid(np.arange(start=x_set[:, 0].min() - 1, stop=x_set[:, 0].max() + 1, step=0.01),
                     np.arange(start=x_set[:, 1].min() - 1, stop=x_set[:, 1].max() + 1, step=0.01))
mtp.contourf(x1, x2, model.predict(np.array([x1.ravel(), x2.ravel()]).T).reshape(x1.shape),
             alpha=0.75, cmap=ListedColormap(('red', 'green')))
mtp.xlim(x1.min(), x1.max())
mtp.ylim(x2.min(), x2.max())
for i, j in enumerate(np.unique(y_set)):
    mtp.scatter(x_set[y_set == j, 0], x_set[y_set == j, 1],
                c=ListedColormap(('red', 'green'))(i), label=j)
mtp.title('K-NN Algorithm (Training set)')
mtp.xlabel('Age')
mtp.ylabel('')
mtp.legend()
mtp.show()
